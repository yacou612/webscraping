{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to company_list.csv\n"
     ]
    }
   ],
   "source": [
    "##Scraping one page\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL of the webpage containing the list of companies\n",
    "url = 'https://www.goafricaonline.com/ci/annuaire/agences-immobilieres'  # Example category URL\n",
    "\n",
    "# Fetch the content of the webpage\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Ensure we notice bad responses\n",
    "\n",
    "# Parse the fetched HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all article elements with data-role=\"company\"\n",
    "articles = soup.find_all('article', {'data-role': 'company'})\n",
    "\n",
    "# Initialize lists to store company names and website links\n",
    "company_data = []\n",
    "\n",
    "# Extract company names and website links\n",
    "for article in articles:\n",
    "    # Find company name within each article\n",
    "    company_name_tag = article.find('a', class_='stretched-link font-bold text-16 t:text-20 text-black hover:text-black no-underline hover:no-underline')\n",
    "    if company_name_tag:\n",
    "        company_name = company_name_tag.get_text(strip=True)\n",
    "    \n",
    "    # Find website link within each article (assuming it's in the data-collect-event-on-click attribute of a div)\n",
    "    div = article.find('div', class_='flex items-center flex-col mm:flex-row t:px-8 z-10 group cursor-pointer no-underline hover:no-underline')\n",
    "    if div:\n",
    "        data_click_attr = div.get('data-collect-event-on-click')\n",
    "        if data_click_attr:\n",
    "            start_index = data_click_attr.find('\"link\":\"') + len('\"link\":\"')\n",
    "            end_index = data_click_attr.find('\"', start_index)\n",
    "            link = data_click_attr[start_index:end_index]\n",
    "            if link.startswith('http'):\n",
    "                # Append the company name and its corresponding link as a tuple\n",
    "                company_data.append((company_name, link))\n",
    "\n",
    "# Print or save company names and website links (example: save to CSV)\n",
    "output_file = 'company_list.csv'\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Company Name', 'Website Link'])\n",
    "    for name, link in company_data:\n",
    "        writer.writerow([name, link])\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to company_list.csv\n"
     ]
    }
   ],
   "source": [
    "#Iterate through all pages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the webpage containing the list of companies\n",
    "base_url = 'https://www.goafricaonline.com/ci/annuaire/agences-immobilieres'\n",
    "\n",
    "# Initialize a list to store company names and website links\n",
    "company_data = []\n",
    "\n",
    "# Iterate through pages 1 to 35\n",
    "for page in range(1, 36):\n",
    "    # Construct the URL for each page\n",
    "    if page == 1:\n",
    "        url = base_url\n",
    "    else:\n",
    "        url = f'{base_url}?p={page}'\n",
    "    \n",
    "    # Fetch the content of the webpage\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we notice bad responses\n",
    "\n",
    "    # Parse the fetched HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all article elements with data-role=\"company\"\n",
    "    articles = soup.find_all('article', {'data-role': 'company'})\n",
    "\n",
    "    # Extract company names and website links\n",
    "    for article in articles:\n",
    "        # Find the company name\n",
    "        company_name_tag = article.find('a', class_='stretched-link font-bold text-16 t:text-20 text-black hover:text-black no-underline hover:no-underline')\n",
    "        if company_name_tag:\n",
    "            company_name = company_name_tag.get_text(strip=True)\n",
    "\n",
    "        # Find the website link\n",
    "        div = article.find('div', class_='flex items-center flex-col mm:flex-row t:px-8 z-10 group cursor-pointer no-underline hover:no-underline')\n",
    "        if div:\n",
    "            data_click_attr = div.get('data-collect-event-on-click')\n",
    "            if data_click_attr:\n",
    "                start_index = data_click_attr.find('\"link\":\"') + len('\"link\":\"')\n",
    "                end_index = data_click_attr.find('\"', start_index)\n",
    "                link = data_click_attr[start_index:end_index]\n",
    "                if link.startswith('http'):\n",
    "                    # Append the company name and its corresponding link as a tuple\n",
    "                    company_data.append((company_name, link))\n",
    "\n",
    "# Save company names and website links to a CSV file\n",
    "output_file = 'company_list.csv'\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Company Name', 'Website Link'])\n",
    "    for name, link in company_data:\n",
    "        writer.writerow([name, link])\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
